{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "# Command to change directory\n",
    "# cd /mnt/(path name)\n",
    "\n",
    "# Command to convert .log.labeled files to .csv\n",
    "# tr '\\t' ',' < \"(name).log.labeled\" | cut -d ',' -f 1-22 > \"(name).csv\"\n",
    "\n",
    "# Replace first 8 lines of .csv file (until #types) with the header below\n",
    "# timestamp,uid,src_ip,src_port,dest_ip,dest_port,protocol,service,duration,orig_bytes,resp_bytes,conn_state,local_orig,local_resp,missed_bytes,history,orig_pkts,orig_ip_bytes,resp_pkts,resp_ip_bytes,label\n",
    "\n",
    "# Load the CSV files\n",
    "benign = pd.read_csv(r\"CSV\\Benign.csv\")\n",
    "torii = pd.read_csv(r\"CSV\\Torii.csv\")\n",
    "mirai = pd.read_csv(r\"CSV\\Mirai.csv\")\n",
    "\n",
    "# Add a label column for each dataset\n",
    "benign['label'] = 0\n",
    "torii['label'] = 1\n",
    "mirai['label'] = 2\n",
    "\n",
    "# Concatenate the datasets\n",
    "data = pd.concat([benign, torii, mirai], axis=0)\n",
    "\n",
    "# Shuffle the dataset\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_ratio = 0.2\n",
    "\n",
    "# Separate the features (X) and the target (y)\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "\n",
    "# Identify numeric columns\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Identify categorical columns that should be encoded\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "X[categorical_cols] = X[categorical_cols].astype(str)\n",
    "\n",
    "# Normalize only the numeric columns\n",
    "scaler = StandardScaler()\n",
    "X_numeric = scaler.fit_transform(X[numeric_cols]) \n",
    "\n",
    "# One-hot encode categorical columns using sklearn\n",
    "encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "X_categorical_sparse = encoder.fit_transform(X[categorical_cols])\n",
    "\n",
    "X_full_sparse = sparse.hstack([sparse.csr_matrix(X_numeric), X_categorical_sparse]).tocsr()\n",
    "\n",
    "# Remove features with low variance\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_encoded = selector.fit_transform(X_full_sparse)\n",
    "print(\"Reduced feature count:\", X_encoded.shape[1])\n",
    "\n",
    "# Convert to dense float32 matrix for ANN\n",
    "X_encoded = X_encoded.astype(np.float32)\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "y_encoded = to_categorical(y, num_classes=3)  # 3 classes: Benign, Torii, Mirai\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=train_test_split_ratio, random_state=42)\n",
    "\n",
    "# Convert y_train to class labels\n",
    "y_train = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Confirm new input dimension\n",
    "input_dim = X_train.shape[1]\n",
    "print(f\"input_dim: {input_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chunk size for processing\n",
    "chunk_size = 1000  # Adjust this based on memory constraints\n",
    "output_dir = \"resampled_chunks\"\n",
    "\n",
    "# Check class distribution before resampling\n",
    "print(\"Class distribution before resampling:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "\n",
    "# Shuffle data to randomize before chunking\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Create a directory to store processed chunks\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define final merged dataset files\n",
    "final_X_file = os.path.join(output_dir, \"final_X.npy\")\n",
    "final_y_file = os.path.join(output_dir, \"final_y.npy\")\n",
    "\n",
    "# Save test data for validation later\n",
    "np.save(os.path.join(output_dir, \"X_test.npy\"), X_test)\n",
    "np.save(os.path.join(output_dir, \"y_test.npy\"), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RandomOverSampler\n",
    "ros = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "chunk_count = 0\n",
    "\n",
    "# Process each chunk\n",
    "for i in range(0, X_train.shape[0], chunk_size):\n",
    "    print(f\"Processing chunk: {i} to {i + chunk_size}...\")\n",
    "    chunk_X = X_train[i:i + chunk_size]\n",
    "    chunk_y = y_train[i:i + chunk_size]\n",
    "    \n",
    "    # Skip chunks with only one class\n",
    "    if len(np.unique(chunk_y)) <= 1:\n",
    "        print(f\"Skipping chunk {i} to {i + chunk_size}: Only one class present.\")\n",
    "        continue\n",
    "    \n",
    "    # Skip empty chunks\n",
    "    if chunk_X.size == 0 or chunk_y.size == 0:\n",
    "        print(f\"Skipping chunk {i} to {i + chunk_size}: No valid data.\")\n",
    "        continue\n",
    "\n",
    "    # Ensure chunk_X has two dimensions\n",
    "    if len(chunk_X.shape) != 2:\n",
    "        print(f\"Skipping chunk {i} to {i + chunk_size}: Unexpected shape {chunk_X.shape}.\")\n",
    "        continue\n",
    "\n",
    "    # Apply RandomOverSampler to the chunk\n",
    "    chunk_X_resampled, chunk_y_resampled = ros.fit_resample(chunk_X, chunk_y)\n",
    "\n",
    "    # Ensure resampled data is not empty\n",
    "    if chunk_X_resampled.size == 0 or chunk_y_resampled.size == 0:\n",
    "        print(f\"Skipping resampled chunk {chunk_count}: No valid data after resampling.\")\n",
    "        continue\n",
    "\n",
    "    # Save resampled chunk as .npy files\n",
    "    np.save(os.path.join(output_dir, f\"chunk_X_{chunk_count}.npy\"), chunk_X_resampled)\n",
    "    np.save(os.path.join(output_dir, f\"chunk_y_{chunk_count}.npy\"), chunk_y_resampled)\n",
    "\n",
    "    # Compress the .npy files after saving using gzip\n",
    "    with open(os.path.join(output_dir, f\"chunk_X_{chunk_count}.npy\"), 'rb') as f_in:\n",
    "        with gzip.open(os.path.join(output_dir, f\"chunk_X_{chunk_count}.npy.gz\"), 'wb') as f_out:\n",
    "            f_out.writelines(f_in)\n",
    "    os.remove(os.path.join(output_dir, f\"chunk_X_{chunk_count}.npy\"))  # Remove the uncompressed file\n",
    "\n",
    "    with open(os.path.join(output_dir, f\"chunk_y_{chunk_count}.npy\"), 'rb') as f_in:\n",
    "        with gzip.open(os.path.join(output_dir, f\"chunk_y_{chunk_count}.npy.gz\"), 'wb') as f_out:\n",
    "            f_out.writelines(f_in)\n",
    "    os.remove(os.path.join(output_dir, f\"chunk_y_{chunk_count}.npy\"))  # Remove the uncompressed file\n",
    "\n",
    "    print(f\"Saved and compressed chunk {chunk_count} with {chunk_X_resampled.shape[0]} samples.\")\n",
    "    chunk_count += 1\n",
    "\n",
    "    # Free memory\n",
    "    del chunk_X, chunk_y, chunk_X_resampled, chunk_y_resampled\n",
    "\n",
    "print(f\"Total chunks saved and compressed: {chunk_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_chunk = True\n",
    "X_shape_total = 0\n",
    "y_shape_total = 0\n",
    "input_dim = None  # Store the number of features\n",
    "\n",
    "# Load and merge all chunks\n",
    "for file in sorted(os.listdir(output_dir)):\n",
    "    if file.startswith(\"chunk_X_\") and file.endswith(\".npy.gz\"):\n",
    "        chunk_index = file.split(\"_\")[-1].split(\".\")[0]\n",
    "        print(f\"Merging chunk {chunk_index}...\")\n",
    "\n",
    "        # Decompress before loading\n",
    "        with gzip.open(os.path.join(output_dir, f\"chunk_X_{chunk_index}.npy.gz\"), 'rb') as f:\n",
    "            chunk_X = np.load(f, allow_pickle=True).item()\n",
    "            chunk_X = chunk_X.toarray()\n",
    "        with gzip.open(os.path.join(output_dir, f\"chunk_y_{chunk_index}.npy.gz\"), 'rb') as f:\n",
    "            chunk_y = np.load(f, allow_pickle=True)\n",
    "\n",
    "        print(f\"Chunk {chunk_index} X shape: {chunk_X.shape}, y shape: {chunk_y.shape}\")\n",
    "\n",
    "        # Skip corrupted or empty chunks\n",
    "        if len(chunk_X.shape) != 2 or chunk_X.shape[0] == 0 or chunk_y.shape[0] == 0:\n",
    "            print(f\"Skipping chunk {chunk_index} due to invalid shape.\")\n",
    "            continue\n",
    "\n",
    "        if first_chunk:\n",
    "            # Set input dimension from the first valid chunk\n",
    "            input_dim = chunk_X.shape[1]\n",
    "\n",
    "            # Initialize memmap with first chunk\n",
    "            X_shape_total = chunk_X.shape[0]\n",
    "            y_shape_total = chunk_y.shape[0]\n",
    "\n",
    "            X_memmap = np.memmap(final_X_file, dtype=chunk_X.dtype, mode=\"w+\", shape=(X_shape_total, input_dim))\n",
    "            y_memmap = np.memmap(final_y_file, dtype=chunk_y.dtype, mode=\"w+\", shape=(y_shape_total,))\n",
    "\n",
    "            X_memmap[:] = chunk_X\n",
    "            y_memmap[:] = chunk_y\n",
    "\n",
    "            first_chunk = False\n",
    "        else:\n",
    "            X_shape_old = X_shape_total  # Store previous shape\n",
    "            y_shape_old = y_shape_total\n",
    "\n",
    "            # Update shape counters\n",
    "            X_shape_total += chunk_X.shape[0]\n",
    "            y_shape_total += chunk_y.shape[0]\n",
    "\n",
    "            print(f\"Before resizing: X shape = ({X_shape_old}, {input_dim}), y shape = ({y_shape_old},)\")\n",
    "\n",
    "            # Resize files before reopening memmap\n",
    "            with open(final_X_file, \"ab\") as f:\n",
    "                f.truncate(X_shape_total * input_dim * chunk_X.itemsize)\n",
    "            with open(final_y_file, \"ab\") as f:\n",
    "                f.truncate(y_shape_total * chunk_y.itemsize)\n",
    "\n",
    "            # Reopen memmap with the new shape\n",
    "            X_memmap = np.memmap(final_X_file, dtype=chunk_X.dtype, mode=\"r+\", shape=(X_shape_total, input_dim))\n",
    "            y_memmap = np.memmap(final_y_file, dtype=chunk_y.dtype, mode=\"r+\", shape=(y_shape_total,))\n",
    "\n",
    "            print(f\"After resizing: X shape = {X_memmap.shape}, y shape = {y_memmap.shape}\")\n",
    "\n",
    "            # Ensure correct index range when appending\n",
    "            X_memmap[X_shape_old:X_shape_total] = chunk_X\n",
    "            y_memmap[y_shape_old:y_shape_total] = chunk_y\n",
    "\n",
    "            print(f\"Chunk {chunk_index} successfully appended.\")\n",
    "\n",
    "        # Ensure data is written to disk\n",
    "        X_memmap.flush()\n",
    "        y_memmap.flush()\n",
    "\n",
    "        del chunk_X, chunk_y  # Free memory\n",
    "        print(f\"Final X shape: {X_memmap.shape}, Final y shape: {y_memmap.shape}\")\n",
    "\n",
    "print(\"All chunks processed and saved to disk!\")\n",
    "\n",
    "X_mmap = np.memmap(final_X_file, dtype=np.float32, mode='r', shape=(X_shape_total, input_dim))\n",
    "y_mmap = np.memmap(final_y_file, dtype=np.int64, mode='r', shape=(y_shape_total,))\n",
    "\n",
    "print(f\"X shape: {X_mmap.shape}\")\n",
    "print(f\"y shape: {y_mmap.shape}\")\n",
    "\n",
    "# Class distribution\n",
    "class_distribution = np.bincount(y_mmap)\n",
    "print(\"Class distribution:\", dict(enumerate(class_distribution)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Dense(512, input_dim=input_dim, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')  # 3 classes: Benign, Torii, Mirai\n",
    "])\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', Precision(), Recall()]\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 3\n",
    "model_save_path = \"models/my_model.h5\"\n",
    "history_save_path = \"metrics/history.pkl\"\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"metrics\", exist_ok=True)\n",
    "\n",
    "# Load memmaps for training\n",
    "X_memmap = np.memmap(final_X_file, dtype=np.float32, mode=\"r\", shape=(X_shape_total, input_dim))\n",
    "y_memmap = np.memmap(final_y_file, dtype=np.int64, mode=\"r\", shape=(y_shape_total,))\n",
    "\n",
    "# Compute class weights\n",
    "class_labels = np.unique(y_memmap)\n",
    "class_weights_array = compute_class_weight(class_weight='balanced', classes=class_labels, y=y_memmap)\n",
    "class_weight_dict = dict(zip(class_labels, class_weights_array))\n",
    "print(\"Class weights:\", class_weight_dict)\n",
    "\n",
    "# Load validation data as memmaps\n",
    "X_test = np.load(os.path.join(output_dir, \"X_test.npy\"), allow_pickle=True)\n",
    "\n",
    "# Unpack if it's a scalar object\n",
    "if X_test.shape == ():\n",
    "    X_test = X_test.item()\n",
    "    if hasattr(X_test, 'toarray'):\n",
    "        X_test = X_test.toarray()\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "y_test = np.load(os.path.join(output_dir, \"y_test.npy\"))\n",
    "y_test = to_categorical(np.argmax(y_test, axis=1) if y_test.ndim == 2 else y_test, num_classes=num_classes)\n",
    "\n",
    "# Train using batch generator\n",
    "def data_generator(X_memmap, y_memmap, batch_size=batch_size, class_weights=None):\n",
    "    indices = np.arange(X_memmap.shape[0])\n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(0, X_memmap.shape[0], batch_size):\n",
    "            batch_idx = indices[i:i+batch_size]\n",
    "            X_batch = X_memmap[batch_idx]\n",
    "            y_batch_int = y_memmap[batch_idx]\n",
    "            y_batch = to_categorical(y_batch_int, num_classes=num_classes)\n",
    "\n",
    "            if class_weights:\n",
    "                sample_weights = np.array([class_weights[cls] for cls in y_batch_int])\n",
    "                yield X_batch, y_batch, sample_weights\n",
    "            else:\n",
    "                yield X_batch, y_batch\n",
    "\n",
    "def val_data_generator(X_val_memmap, y_val_memmap, batch_size=batch_size):\n",
    "    indices = np.arange(X_val_memmap.shape[0])\n",
    "    while True:\n",
    "        for i in range(0, X_val_memmap.shape[0], batch_size):\n",
    "            batch_idx = indices[i:i + batch_size]\n",
    "            X_batch = X_val_memmap[batch_idx]\n",
    "            y_batch = y_val_memmap[batch_idx]\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Model Training\n",
    "history = model.fit(\n",
    "    data_generator(X_memmap, y_memmap, batch_size, class_weights=class_weight_dict),\n",
    "    steps_per_epoch=X_memmap.shape[0] // batch_size,\n",
    "    epochs=100,\n",
    "    validation_data=val_data_generator(X_test, y_test, batch_size),\n",
    "    validation_steps=X_test.shape[0] // batch_size,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "# Save model and history\n",
    "model.save(model_save_path)\n",
    "with open(history_save_path, \"wb\") as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, average_precision_score, accuracy_score\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report (Precision, Recall, F1-score)\n",
    "print(\"Final Classification Report:\")\n",
    "print(classification_report(y_true_labels, y_pred_labels))\n",
    "\n",
    "accuracy = accuracy_score(y_true_labels, y_pred_labels)\n",
    "print(f\"\\nFinal Accuracy on Test Set: {accuracy:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Benign', 'Torii', 'Mirai'])\n",
    "disp.plot(cmap='viridis')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Normalized Confusion Matrix\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_normalized, annot=True, cmap='Blues', fmt='.2f',\n",
    "            xticklabels=['Benign', 'Torii', 'Mirai'],\n",
    "            yticklabels=['Benign', 'Torii', 'Mirai'])\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision = {}\n",
    "recall = {}\n",
    "average_precision = {}\n",
    "for i in range(num_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(y_test[:, i], y_pred[:, i])\n",
    "    plt.plot(recall[i], precision[i], marker='.', label=f'Class {i} (AP={average_precision[i]:.2f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves for Each Class')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Summary Table\n",
    "print(f\"\\n{'Class':<10}{'Precision':<10}{'Recall':<10}{'F1-score':<10}{'AP':<10}\")\n",
    "report = classification_report(y_true_labels, y_pred_labels, output_dict=True)\n",
    "for i in range(num_classes):\n",
    "    p = report[str(i)]['precision']\n",
    "    r = report[str(i)]['recall']\n",
    "    f1 = report[str(i)]['f1-score']\n",
    "    ap = average_precision[i]\n",
    "    print(f\"{i:<10}{p:<10.2f}{r:<10.2f}{f1:<10.2f}{ap:<10.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
