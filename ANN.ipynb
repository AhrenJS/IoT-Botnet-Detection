{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, average_precision_score, precision_recall_fscore_support\n",
    "\n",
    "# Command to convert .log.labeled files to .csv\n",
    "# tr '\\t' ',' < \"(name).log.labeled\" | cut -d ',' -f 1-22 > \"(name).csv\"\n",
    "\n",
    "# Command to change directory\n",
    "# cd /mnt/(path name)\n",
    "\n",
    "# Replace first 8 lines of .csv file (until #types) with the header below\n",
    "# timestamp,uid,src_ip,src_port,dest_ip,dest_port,protocol,service,duration,orig_bytes,resp_bytes,conn_state,local_orig,local_resp,missed_bytes,history,orig_pkts,orig_ip_bytes,resp_pkts,resp_ip_bytes,label\n",
    "\n",
    "# Load the CSV files\n",
    "benign = pd.read_csv(r\"CSV\\Benign.csv\")\n",
    "torii = pd.read_csv(r\"CSV\\Torii.csv\")\n",
    "mirai = pd.read_csv(r\"CSV\\Mirai.csv\")\n",
    "\n",
    "# Add a label column for each dataset\n",
    "benign['label'] = 0\n",
    "torii['label'] = 1\n",
    "mirai['label'] = 2\n",
    "\n",
    "# Concatenate the datasets\n",
    "data = pd.concat([benign, torii, mirai], axis=0)\n",
    "\n",
    "# Shuffle the dataset\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features (X) and the target (y)\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "\n",
    "# Identify numeric columns\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Identify categorical columns that should be encoded\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Label encode categorical columns\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    X[col] = label_encoder.fit_transform(X[col].astype(str))\n",
    "\n",
    "# Normalize only the numeric columns\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()  # Make a copy of the encoded DataFrame\n",
    "X_scaled[numeric_cols] = scaler.fit_transform(X_scaled[numeric_cols])\n",
    "\n",
    "# One-hot encode with sparse matrix\n",
    "X_encoded = pd.get_dummies(X_scaled, columns=categorical_cols, sparse=True, drop_first=True)\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "y_encoded = to_categorical(y, num_classes=3)  # 3 classes: Benign, Torii, Mirai\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution before resampling\n",
    "print(\"Original training set class distribution:\")\n",
    "print(pd.Series(np.argmax(y_train, axis=1)).value_counts())\n",
    "\n",
    "# Shuffle data to randomize before chunking\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Define chunk size for processing\n",
    "chunk_size = 1000  # Adjust this based on memory constraints\n",
    "\n",
    "# Create a directory to store processed chunks\n",
    "output_dir = \"resampled_chunks\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define final merged dataset files\n",
    "final_X_file = os.path.join(output_dir, \"final_X.npy\")\n",
    "final_y_file = os.path.join(output_dir, \"final_y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RandomOverSampler\n",
    "ros = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "chunk_count = 0\n",
    "\n",
    "# Process each chunk\n",
    "for i in range(0, X_train.shape[0], chunk_size):\n",
    "    print(f\"Processing chunk: {i} to {i + chunk_size}...\")\n",
    "    chunk_X = X_train[i:i + chunk_size]\n",
    "    chunk_y = np.argmax(y_train[i:i + chunk_size], axis=1)\n",
    "    \n",
    "    # Skip chunks with only one class\n",
    "    if len(np.unique(chunk_y)) <= 1:\n",
    "        print(f\"Skipping chunk {i} to {i + chunk_size}: Only one class present.\")\n",
    "        continue\n",
    "    \n",
    "    # Apply RandomOverSampler to the chunk\n",
    "    chunk_X_resampled, chunk_y_resampled = ros.fit_resample(chunk_X, chunk_y)\n",
    "    \n",
    "    # Save resampled chunk as .npy files\n",
    "    np.save(os.path.join(output_dir, f\"chunk_X_{chunk_count}.npy\"), chunk_X_resampled)\n",
    "    np.save(os.path.join(output_dir, f\"chunk_y_{chunk_count}.npy\"), chunk_y_resampled)\n",
    "    \n",
    "    # Compress the .npy files after saving using gzip\n",
    "    with open(os.path.join(output_dir, f\"chunk_X_{chunk_count}.npy\"), 'rb') as f_in:\n",
    "        with gzip.open(os.path.join(output_dir, f\"chunk_X_{chunk_count}.npy.gz\"), 'wb') as f_out:\n",
    "            f_out.writelines(f_in)\n",
    "    os.remove(os.path.join(output_dir, f\"chunk_X_{chunk_count}.npy\"))  # Remove the uncompressed file\n",
    "\n",
    "    with open(os.path.join(output_dir, f\"chunk_y_{chunk_count}.npy\"), 'rb') as f_in:\n",
    "        with gzip.open(os.path.join(output_dir, f\"chunk_y_{chunk_count}.npy.gz\"), 'wb') as f_out:\n",
    "            f_out.writelines(f_in)\n",
    "    os.remove(os.path.join(output_dir, f\"chunk_y_{chunk_count}.npy\"))  # Remove the uncompressed file\n",
    "    \n",
    "    print(f\"Saved and compressed chunk {chunk_count} with {chunk_X_resampled.shape[0]} samples.\")\n",
    "    chunk_count += 1\n",
    "\n",
    "    # Free memory\n",
    "    del chunk_X, chunk_y, chunk_X_resampled, chunk_y_resampled\n",
    "\n",
    "print(f\"Total chunks saved and compressed: {chunk_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_chunk = True\n",
    "X_shape_total = None  # Use `None` to detect first chunk dynamically\n",
    "y_shape_total = None\n",
    "\n",
    "# Load and merge all chunks using np.memmap\n",
    "for file in sorted(os.listdir(output_dir)):  \n",
    "    if file.startswith(\"chunk_X_\") and file.endswith(\".npy.gz\"):\n",
    "        chunk_index = file.split(\"_\")[-1].split(\".\")[0]  # Extract index number\n",
    "        print(f\"Merging chunk {chunk_index}...\")\n",
    "\n",
    "        # Decompress before loading\n",
    "        with gzip.open(os.path.join(output_dir, f\"chunk_X_{chunk_index}.npy.gz\"), 'rb') as f:\n",
    "            chunk_X = np.load(f)\n",
    "        with gzip.open(os.path.join(output_dir, f\"chunk_y_{chunk_index}.npy.gz\"), 'rb') as f:\n",
    "            chunk_y = np.load(f)\n",
    "\n",
    "        if first_chunk:\n",
    "            # Initialize `memmap` with first chunk's shape\n",
    "            X_shape_total = chunk_X.shape[0]\n",
    "            y_shape_total = chunk_y.shape[0]\n",
    "\n",
    "            X_memmap = np.memmap(final_X_file, dtype=chunk_X.dtype, mode=\"w+\", shape=(X_shape_total, chunk_X.shape[1]))\n",
    "            y_memmap = np.memmap(final_y_file, dtype=chunk_y.dtype, mode=\"w+\", shape=(y_shape_total,))\n",
    "\n",
    "            X_memmap[:] = chunk_X\n",
    "            y_memmap[:] = chunk_y\n",
    "            first_chunk = False\n",
    "        else:\n",
    "            # Update total size before resizing\n",
    "            X_shape_old = X_shape_total\n",
    "            y_shape_old = y_shape_total\n",
    "            X_shape_total += chunk_X.shape[0]\n",
    "            y_shape_total += chunk_y.shape[0]\n",
    "\n",
    "            # Resize `memmap` to accommodate new data\n",
    "            X_memmap.flush()\n",
    "            y_memmap.flush()\n",
    "\n",
    "            X_memmap = np.memmap(final_X_file, dtype=chunk_X.dtype, mode=\"r+\", shape=(X_shape_total, chunk_X.shape[1]))\n",
    "            y_memmap = np.memmap(final_y_file, dtype=chunk_y.dtype, mode=\"r+\", shape=(y_shape_total,))\n",
    "\n",
    "            # Append new data correctly\n",
    "            X_memmap[X_shape_old:] = chunk_X\n",
    "            y_memmap[y_shape_old:] = chunk_y\n",
    "\n",
    "        del chunk_X, chunk_y  # Free memory\n",
    "\n",
    "print(\"All chunks processed and saved to disk!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataset loading\n",
    "print(\"Loading final dataset into memory (only when needed)...\")\n",
    "X_train_resampled = np.memmap(final_X_file, dtype=np.float64, mode=\"r\")\n",
    "y_train_resampled = np.memmap(final_y_file, dtype=np.int64, mode=\"r\")\n",
    "\n",
    "# Check class distribution after resampling\n",
    "print(\"Resampled training set class distribution:\")\n",
    "print(pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "# Convert back to one-hot encoding after resampling\n",
    "y_train_resampled_encoded = to_categorical(y_train_resampled, num_classes=3)\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_resampled), y=y_train_resampled)\n",
    "class_weight_dict = dict(zip(np.unique(y_train_resampled), class_weights))\n",
    "\n",
    "print(\"Class weights:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Dense(512, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')  # 3 classes: Benign, Torii, Mirai\n",
    "])\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_resampled,\n",
    "    y_train_resampled_encoded,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report (Precision, Recall, F1-score)\n",
    "print(\"Final Classification Report:\")\n",
    "print(classification_report(y_true_labels, y_pred_labels))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Benign', 'Torii', 'Mirai'])\n",
    "disp.plot(cmap='viridis')\n",
    "plt.show()\n",
    "\n",
    "# Calculate precision-recall curves and average precision scores\n",
    "precision = {}\n",
    "recall = {}\n",
    "average_precision = {}\n",
    "\n",
    "for i in range(3):  # 3 classes: 0 (Benign), 1 (Torii), 2 (Mirai)\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_pred[:, i])  # OvR for each class\n",
    "    average_precision[i] = average_precision_score(y_test[:, i], y_pred[:, i])\n",
    "\n",
    "# Plot precision-recall curves for each class\n",
    "for i in range(3):\n",
    "    plt.plot(recall[i], precision[i], marker='.', label=f'Class {i} (AP={average_precision[i]:.2f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves for Each Class')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print average precision scores for each class\n",
    "for i in range(3):\n",
    "    print(f'Average Precision for Class {i}: {average_precision[i]:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
